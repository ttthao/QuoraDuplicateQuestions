{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "import string\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams[\"figure.figsize\"] = (10,5)\n",
    "\n",
    "from sklearn import svm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data...\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "print(\"Reading data...\" )    \n",
    "        \n",
    "with open('questions.csv') as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    keys = next(reader)\n",
    "    data = [row for row in reader]  \n",
    "    \n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def vectorize_string(s):\n",
    "    '''\n",
    "    turns a string into a dictionary of words. \n",
    "    Can be used as a 'vector' where noexistant keys have val 0\n",
    "    \n",
    "    Each (substrings of len p) maps to the amount of appearances\n",
    "    of itself in the original string\n",
    "    '''\n",
    "    \n",
    "    v = {}\n",
    "    for word in s.split():\n",
    "        if word in v:\n",
    "            v[word] += 1\n",
    "        else:\n",
    "            v[word] = 1\n",
    "            \n",
    "    return v\n",
    "\n",
    "#indexes for values in data\n",
    "pid = 0\n",
    "qid1 = 1\n",
    "qid2 = 2\n",
    "q1 = 3\n",
    "q2 = 4\n",
    "dup = 5\n",
    "\n",
    "punctuation = set(string.punctuation)\n",
    "for d in data:\n",
    "    d[q1] = ''.join([c for c in d[q1].lower() if not c in punctuation])\n",
    "    d[q2] = ''.join([c for c in d[q2].lower() if not c in punctuation])\n",
    "\n",
    "\n",
    "uniq_questions = {} # list of unique questions strings\n",
    "\n",
    "for d in data:\n",
    "    if d[q1] in uniq_questions:\n",
    "        uniq_questions[d[q1]] += 1\n",
    "    else:\n",
    "        uniq_questions[d[q1]] = 1\n",
    "    \n",
    "    if d[q2] in uniq_questions:\n",
    "        uniq_questions[d[q2]] += 1\n",
    "    else:\n",
    "        uniq_questions[d[q2]] = 1\n",
    "        \n",
    "'''\n",
    "Uses all unique questions as set of documents\n",
    "idf = log10(document count / document count for word)\n",
    "'''\n",
    "gidf = {}\n",
    "for q in uniq_questions:\n",
    "    d = vectorize_string(q)\n",
    "    for word in d:\n",
    "        if word in gidf:\n",
    "            gidf[word] += 1\n",
    "        else:\n",
    "            gidf[word] = 1\n",
    "\n",
    "for word in gidf:\n",
    "    gidf[word] = math.log10(len(uniq_questions)/gidf[word])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# transform data into useable formats\n",
    "# [int,int,int,dict,dict,int]\n",
    "\n",
    "for i in range(len(data)):\n",
    "    data[i][pid] = int(data[i][pid])\n",
    "    data[i][qid1] = int(data[i][qid1])\n",
    "    data[i][qid2] = int(data[i][qid2])\n",
    "    data[i][q1] = vectorize_string(data[i][q1])\n",
    "    data[i][q2] = vectorize_string(data[i][q2])\n",
    "    data[i][dup] = int(data[i][dup])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def magnitude_dict(data):\n",
    "    '''\n",
    "    Evaulats the magnitude of a dict\n",
    "    '''\n",
    "    \n",
    "    return sum([value**2 for value in data.values()])**.5\n",
    "\n",
    "\n",
    "def dot_dicts(d1,d2):\n",
    "    '''\n",
    "    multiplies the values of the overlaping keys\n",
    "    in two dictionaries.  It chooses to iterate\n",
    "    over the smaller and lookup in the larger\n",
    "    '''\n",
    "    \n",
    "    a = d1 if len(d1) < len(d2) else d2\n",
    "    b = d2 if len(d1) < len(d2) else d1\n",
    "    \n",
    "    dot = 0\n",
    "    # a is shorter than b\n",
    "    for s in a:\n",
    "        if s in b:\n",
    "            dot += a[s]*b[s]\n",
    "            \n",
    "    return dot\n",
    "    \n",
    "    \n",
    "def cosine_simularity_dicts(d1,d2):\n",
    "    '''\n",
    "    returns the cosine simularity of two strings\n",
    "    words in each string are considered dimensions\n",
    "    '''\n",
    "\n",
    "    if (len(d1) == 0 and len(d2) == 0):\n",
    "        return 1\n",
    "    elif (len(d1) == 0 or len(d2) == 0):\n",
    "        return 0\n",
    "\n",
    "    return dot_dicts(d1,d2)/(magnitude_dict(d1)*magnitude_dict(d2))\n",
    "\n",
    "\n",
    "def intersect_dicts(d1,d2):\n",
    "    '''\n",
    "    returns an integer count of the intesecting keys in \n",
    "    two dictionaries\n",
    "    '''\n",
    "    \n",
    "    a = d1 if len(d1) < len(d2) else d2\n",
    "    b = d2 if len(d1) < len(d2) else d1\n",
    "    \n",
    "    count = 0\n",
    "    for s in a:\n",
    "        if s in b:\n",
    "            count += 1\n",
    "            \n",
    "    return count\n",
    "\n",
    "\n",
    "def union_dicts(d1,d2):\n",
    "    '''\n",
    "    returns an integer count of the union of keys in \n",
    "    two dictionaries\n",
    "    '''\n",
    "    \n",
    "    shared = 0\n",
    "    unique = 0\n",
    "    \n",
    "    for s in d1:\n",
    "        if s in d2:\n",
    "            shared += 1\n",
    "        else:\n",
    "            unique += 1\n",
    "    \n",
    "    for s in d2:\n",
    "        if s not in d1:\n",
    "            unique += 1\n",
    "            \n",
    "    return shared + unique \n",
    "\n",
    "    \n",
    "def jaccard_simularity_dicts(d1,d2):\n",
    "    '''\n",
    "    returns the jaccard simularity of two strings\n",
    "    words in each strings are considered dimensions\n",
    "    '''\n",
    "    \n",
    "    if (len(d1) == 0 and len(d2) == 0):\n",
    "        return 1\n",
    "    elif (len(d1) == 0 or len(d2) == 0):\n",
    "        return 0\n",
    "\n",
    "    return float(intersect_dicts(d1,d2))/union_dicts(d1,d2)\n",
    "\n",
    "\n",
    "def sum_tf_idf(d1,d2):\n",
    "    '''\n",
    "    returns the sum of the tf-idf of each word in two dicts\n",
    "    '''\n",
    "    \n",
    "    idf = {}\n",
    "    for word in d1:\n",
    "        if word in d1 and word in d2:\n",
    "            idf[word] = 0\n",
    "        else:\n",
    "            idf[word] = 0.3010299956639812 # hard coded log10(2)\n",
    "    for word in d2:\n",
    "        if word not in idf:\n",
    "            idf[word] = 0.3010299956639812\n",
    "    \n",
    "    tf_idf = {}\n",
    "    for word in d1:\n",
    "        tf_idf[word] = float(d1[word])/len(d1)*idf[word]\n",
    "    for word in d2:\n",
    "        if word not in tf_idf:\n",
    "            tf_idf[word] = float(d2[word])/len(d2)*idf[word]\n",
    "            \n",
    "    total = 0\n",
    "    for word in tf_idf:\n",
    "        total += tf_idf[word]\n",
    "        \n",
    "    return total\n",
    "\n",
    "\n",
    "def sum_tf_gidf(d1,d2):\n",
    "    tf_gidf = {}\n",
    "    for word in d1:\n",
    "        tf_gidf[word] = float(d1[word])/len(d1)*gidf[word]\n",
    "    for word in d2:\n",
    "        if word not in tf_gidf:\n",
    "            tf_gidf[word] = float(d2[word])/len(d2)*gidf[word]\n",
    "            \n",
    "    total = 0\n",
    "    for word in tf_gidf:\n",
    "        total += tf_gidf[word]\n",
    "        \n",
    "    return total\n",
    "\n",
    "\n",
    "def sum_tf_idf_gidf(d1,d2):\n",
    "    '''\n",
    "    returns the sum of the tf-idf of each word in\n",
    "    both strings\n",
    "    '''\n",
    "    \n",
    "    idf = {}\n",
    "    for word in d1:\n",
    "        if word in d1 and word in d2:\n",
    "            idf[word] = 0\n",
    "        else:\n",
    "            idf[word] = 0.3010299956639812\n",
    "    for word in d2:\n",
    "        if word not in idf:\n",
    "            idf[word] = 0.3010299956639812\n",
    "    \n",
    "    tf_idf = {}\n",
    "    for word in d1:\n",
    "        tf_idf[word] = float(d1[word])/len(d1)*idf[word]*gidf[word]\n",
    "    for word in d2:\n",
    "        if word not in tf_idf:\n",
    "            tf_idf[word] = float(d2[word])/len(d2)*idf[word]*gidf[word]\n",
    "            \n",
    "    total = 0\n",
    "    for word in tf_idf:\n",
    "        total += tf_idf[word]\n",
    "        \n",
    "    return total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def feature_vector(point):\n",
    "    d1 = point[q1]\n",
    "    d2 = point[q2]\n",
    "    \n",
    "    v = [ \n",
    "        len(point[q1]),\n",
    "        len(point[q2]),\n",
    "        union_dicts(d1,d2),\n",
    "        intersect_dicts(d1,d2),\n",
    "        cosine_simularity_dicts(d1,d2),\n",
    "        jaccard_simularity_dicts(d1,d2),\n",
    "        sum_tf_idf(d1,d2),\n",
    "        sum_tf_gidf(d1,d2),\n",
    "        sum_tf_idf_gidf(d1,d2),\n",
    "    ]\n",
    "    \n",
    "    return v\n",
    "\n",
    "X = [feature_vector(d) for d in data]\n",
    "y = [d[dup] for d in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 250k training\n",
    "X_train = X[:1000]\n",
    "y_train = y[:1000]\n",
    "\n",
    "# 100k validate\n",
    "X_valid = X[250000:350000]\n",
    "y_valid = y[250000:350000]\n",
    "\n",
    "# 54351 test\n",
    "X_test = X[350000:]\n",
    "y_test = y[350000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def accuracy(prediction,actual):\n",
    "    correct = 0\n",
    "    for p, a in zip(prediction, actual):\n",
    "        if (p == a):\n",
    "            correct += 1\n",
    "    return float(correct)/len(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| 0.01 | 0.62 | 0.62962 |\n",
      "| 0.1 | 0.682 | 0.66529 |\n",
      "| 1 | 0.746 | 0.68287 |\n",
      "| 10 | 0.828 | 0.66551 |\n",
      "| 100 | 0.897 | 0.64934 |\n"
     ]
    }
   ],
   "source": [
    "clf = {}\n",
    "for i in range(-2,3):\n",
    "    C = 10**i\n",
    "    clf[C] = svm.SVC(C=C)\n",
    "    clf[C].fit(X_train, y_train)\n",
    "\n",
    "    train_predictions = clf[C].predict(X_train)\n",
    "    valid_predictions = clf[C].predict(X_valid)\n",
    "    \n",
    "    print('|',C, '|',accuracy(train_predictions, y_train), '|',accuracy(valid_predictions, y_valid), '|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
